import librosa
import numpy as np
import soundfile as sf
import os
from typing import List, Tuple
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

def find_silence_segments(audio: np.ndarray, sr: int, 
                         min_silence_duration: float = 0.5,
                         silence_threshold: float = 0.01) -> List[Tuple[float, float]]:
    """
    æŸ¥æ‰¾éŸ³é¢‘ä¸­çš„é™éŸ³æ®µ
    Returns: é™éŸ³æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(å¼€å§‹æ—¶é—´, ç»“æŸæ—¶é—´)
    """
    energy = librosa.feature.rms(y=audio)[0]
    silent_frames = energy < silence_threshold
    silent_segments = []
    start_frame = None
    for i, is_silent in enumerate(silent_frames):
        if is_silent and start_frame is None:
            start_frame = i
        elif not is_silent and start_frame is not None:
            duration = (i - start_frame) * 512 / sr
            if duration >= min_silence_duration:
                silent_segments.append((start_frame * 512 / sr, i * 512 / sr))
            start_frame = None
    # å¤„ç†ç»“å°¾é™éŸ³
    if start_frame is not None:
        duration = (len(silent_frames) - start_frame) * 512 / sr
        if duration >= min_silence_duration:
            silent_segments.append((start_frame * 512 / sr, len(silent_frames) * 512 / sr))
    return silent_segments

def find_best_split_point(audio: np.ndarray, sr: int, target_time: float, search_window: float = 30.0) -> float:
    """
    åœ¨ç›®æ ‡æ—¶é—´ç‚¹é™„è¿‘æ‰¾åˆ°æœ€ä½³åˆ†å‰²ç‚¹ï¼ˆä¼˜å…ˆé™éŸ³åŒºï¼Œå¦åˆ™èƒ½é‡æœ€ä½ç‚¹ï¼‰
    Args:
        audio: éŸ³é¢‘æ•°æ®
        sr: é‡‡æ ·ç‡
        target_time: ç›®æ ‡åˆ†å‰²æ—¶é—´ç‚¹
        search_window: æœç´¢çª—å£å¤§å°ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
    Returns: æœ€ä½³åˆ†å‰²ç‚¹çš„æ—¶é—´æˆ³
    """
    start_frame = max(0, int((target_time - search_window) * sr))
    end_frame = min(len(audio), int((target_time + search_window) * sr))
    search_audio = audio[start_frame:end_frame]
    silent_segments = find_silence_segments(search_audio, sr, min_silence_duration=1.0, silence_threshold=0.015)
    if silent_segments:
        # æ‰¾åˆ°æœ€æ¥è¿‘ç›®æ ‡æ—¶é—´çš„é™éŸ³æ®µ
        best_segment = min(silent_segments, key=lambda x: abs((x[0] + x[1])/2 - search_window))
        # è¿”å›é™éŸ³æ®µçš„ä¸­é—´ç‚¹ï¼ˆåŠ ä¸Šçª—å£èµ·ç‚¹ï¼‰
        return (best_segment[0] + best_segment[1]) / 2 + (target_time - search_window)
    # æ²¡æœ‰é™éŸ³æ®µï¼Œæ‰¾èƒ½é‡æœ€ä½ç‚¹
    energy = librosa.feature.rms(y=search_audio)[0]
    min_energy_frame = np.argmin(energy)
    return (start_frame + min_energy_frame * 512) / sr

def split_audio_file(input_file: str, output_dir: str, num_parts: int = 4) -> List[str]:
    """
    æ™ºèƒ½åˆ†å‰²éŸ³é¢‘æ–‡ä»¶ï¼Œåˆ†å‰²ç‚¹ä¼˜å…ˆé€‰é™éŸ³åŒº
    Returns: åˆ†å‰²åçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"ğŸ“– åŠ è½½éŸ³é¢‘æ–‡ä»¶: {input_file}")
    audio, sr = librosa.load(input_file, sr=None)
    duration = len(audio) / sr
    target_length = duration / num_parts
    logger.info(f"â±ï¸ éŸ³é¢‘æ€»æ—¶é•¿: {duration:.1f}ç§’")
    logger.info(f"ğŸ“Š ç›®æ ‡åˆ†ç‰‡é•¿åº¦: {target_length:.1f}ç§’")
    
    # æ‰¾åˆ°åˆ†å‰²ç‚¹
    split_points = [0]
    for i in range(1, num_parts):
        target_time = i * target_length
        split_point = find_best_split_point(audio, sr, target_time, search_window=30.0)
        split_points.append(split_point)
    split_points.append(duration)
    
    # éªŒè¯åˆ†å‰²ç‚¹
    total_duration = 0
    for i in range(len(split_points)-1):
        segment_duration = split_points[i+1] - split_points[i]
        total_duration += segment_duration
        logger.info(f"åˆ†ç‰‡ {i+1} æ—¶é•¿: {segment_duration:.1f}ç§’")
    
    # éªŒè¯æ€»æ—¶é•¿
    if abs(total_duration - duration) > 0.1:  # å…è®¸0.1ç§’çš„è¯¯å·®
        logger.error(f"âŒ åˆ†ç‰‡æ€»æ—¶é•¿ ({total_duration:.1f}ç§’) ä¸åŸéŸ³é¢‘æ—¶é•¿ ({duration:.1f}ç§’) ä¸åŒ¹é…ï¼")
        raise ValueError("åˆ†ç‰‡æ€»æ—¶é•¿ä¸åŸéŸ³é¢‘æ—¶é•¿ä¸åŒ¹é…")
    logger.info(f"âœ… åˆ†ç‰‡æ€»æ—¶é•¿éªŒè¯é€šè¿‡: {total_duration:.1f}ç§’")
    
    # åˆ†å‰²å¹¶ä¿å­˜éŸ³é¢‘
    output_files = []
    for i in range(num_parts):
        start_frame = int(split_points[i] * sr)
        end_frame = int(split_points[i+1] * sr)
        segment = audio[start_frame:end_frame]
        base_name = os.path.splitext(os.path.basename(input_file))[0]
        output_file = os.path.join(output_dir, f"{base_name}_part{i+1}.wav")
        sf.write(output_file, segment, sr)
        output_files.append(output_file)
        logger.info(f"ğŸ’¾ ä¿å­˜åˆ†ç‰‡ {i+1}: {output_file}")
        logger.info(f"â±ï¸ åˆ†ç‰‡æ—¶é•¿: {len(segment)/sr:.1f}ç§’")
        logger.info(f"ğŸ“ æ—¶é—´æˆ³: {split_points[i]:.1f}s - {split_points[i+1]:.1f}s")
    
    return output_files

def main():
    import argparse
    parser = argparse.ArgumentParser(description='åˆ†å‰²éŸ³é¢‘æ–‡ä»¶')
    parser.add_argument('--input', required=True, help='è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--num-parts', type=int, default=4, help='åˆ†å‰²æ•°é‡')
    args = parser.parse_args()
    split_audio_file(args.input, args.output_dir, args.num_parts)

if __name__ == '__main__':
    main() 